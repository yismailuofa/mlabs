{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In the world of natural language processing (NLP), models like ChatGPT have become household names. These models are pre-trained on vast amounts of text data up to a certain point in time, known as their \"knowledge cutoff.\" While incredibly versatile, their static nature means they can't incorporate information or events that occur after this cutoff. This is where Retrieval-Augmented Generation (RAG) comes in, blending the generative capabilities of models like ChatGPT with the dynamic, up-to-date knowledge from external sources.\n",
        "\n",
        "\n",
        "### How RAG Works\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![RAG](https://taesiri.xyz/data/rag2.png)\n",
        "\n",
        "\n",
        "\n",
        "RAG enhances traditional language models through a two-stage process:\n",
        "\n",
        "1. **Retrieval Stage**: The system queries a continuously updated database or knowledge base to find information relevant to the input query. This allows the model to access the most current data, even if it's beyond its original training cutoff.\n",
        "\n",
        "2. **Generation Stage**: Leveraging a generative model (e.g., GPT), RAG integrates the context from the retrieved documents to produce informed and relevant text. This step ensures that the generation is not only based on the model's pre-trained knowledge but is also augmented with the latest information.\n",
        "\n",
        "### Key Applications\n",
        "\n",
        "- **Question Answering**: RAG systems can answer questions with the most current information, overcoming the knowledge cutoff limitation of standalone generative models.\n",
        "- **Conversational Agents**: Chatbots powered by RAG can provide users with up-to-date answers, making them more useful for current events and news-related queries.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- **Current Information**: RAG allows language models to break free from their knowledge cutoff, making them more relevant for today's rapidly changing world.\n",
        "- **Depth and Accuracy**: The retrieval component ensures that the generated content is not only contextually relevant but also deeply informative and factually accurate.\n",
        "- **Adaptability**: By changing the external data sources, RAG can be tailored to different domains and information needs.\n",
        "\n",
        "### [Llamaindex](https://www.llamaindex.ai/)\n",
        "\n",
        "LlamaIndex is a versatile data framework designed to connect custom data sources with large language models (LLMs) like GPT-4. It serves as a bridge between enterprise data and LLM applications, enabling the ingestion, structuring, retrieval, and integration of data for various applications. LlamaIndex allows for the loading of data from over 160 sources in different formats, indexing this data for diverse use cases, and orchestrating LLM workflows efficiently. It offers a comprehensive suite of modules to evaluate LLM application performance and seamlessly integrates with observability partners. Additionally, LlamaIndex boasts a thriving developer network, community contributions, and integration options with various services.\n",
        "\n",
        "\n",
        "### Project\n",
        "\n",
        "In this notebook, we'll guide you through the process of leveraging LlamaIndex to enhance information retrieval and text generation. Specifically, we will demonstrate how to use LlamaIndex to upload a PDF document and dissect it into manageable segments. These segments will then be systematically stored in a vector database, designed for efficient querying. When a query is submitted, our system will search this database to find the most relevant document segments. The most pertinent segment – or `chunk` – will be retrieved as the context to address the query. Subsequently, this context will be provided to a compact language model, in this case, [Microsoft Phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/). We'll then instruct Phi-2 to craft a response, drawing upon both the specific question posed and the context supplied by the selected document chunk. This method showcases the synergy between advanced retrieval techniques and modern language models to generate informed, contextually relevant responses.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HFUHm2AzmAqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies\n",
        "\n",
        "(This step might take longer than 10 minutes)"
      ],
      "metadata": {
        "id": "BR9hy0HOpLJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q llama-index\n",
        "!pip install -q gradio\n",
        "!pip install -q einops\n",
        "!pip install -q accelerate\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q llama-index-embeddings-instructor\n",
        "!pip install -q llama-index-llms-huggingface\n",
        "!pip install -q llama-index-llms-openai"
      ],
      "metadata": {
        "id": "3kiH95SuTwGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "import torch"
      ],
      "metadata": {
        "id": "vs122vqGTsBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Data"
      ],
      "metadata": {
        "id": "QZf_IuT6xb3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Upload a single PDF document into Data folder\n",
        "\n",
        "# we will use SimpleDirectoryReader to load all the documents in a folder\n",
        "documents = SimpleDirectoryReader(\"./Data\").load_data()"
      ],
      "metadata": {
        "id": "B4f54p8SnowD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "29LUEgOFWZF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1009aca8-aa0f-41ee-b1b9-9f04a3b0f89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
        "query_wrapper_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReohpVquTmeE",
        "outputId": "90b2d841-9081-4255-a59a-788215d21bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='<|USER|>{query_str}<|ASSISTANT|>')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"microsoft/phi-2\",\n",
        "    model_name=\"microsoft/phi-2\",\n",
        "    device_map=\"cuda\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")\n"
      ],
      "metadata": {
        "id": "_crtBRlwYFeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# loads BAAI/bge-small-en-v1.5 from huggingface for embedding - https://huggingface.co/BAAI/bge-small-en-v1.5\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "7x13lpwiTiXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an vector database from document chunks\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Query the database and return the most relevant conent to the query\n",
        "def predict(input, history):\n",
        "  response = query_engine.query(input)\n",
        "  return str(response)\n"
      ],
      "metadata": {
        "id": "IlLCVBsbTdQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Try querying the engine with multiple question and examine the response and source_nodes\n",
        "r = query_engine.query(\"YOUR QUESTION HERE\")"
      ],
      "metadata": {
        "id": "ZuQ2sX_EZ7y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examine the response\n",
        "r"
      ],
      "metadata": {
        "id": "mNJGKAACR4uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examine the source nodes used for the answer\n",
        "r.source_nodes[0]"
      ],
      "metadata": {
        "id": "hwZ9ZOazR1Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJRCXoSGaZzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a Chat Interface\n",
        "\n",
        "Below, we have created a chat interface that allows you to ask various questions based on the document stored in it. Please use this chat application to ask 10 different questions and then report your understanding in the cell below."
      ],
      "metadata": {
        "id": "pmGLOm2NzdQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(predict).launch(share=True)"
      ],
      "metadata": {
        "id": "ewhSOeV0oofp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer and analysis here."
      ],
      "metadata": {
        "id": "vbAwVOmEk9Gz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}